{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import interp\n",
    "from cycler import cycler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from string import punctuation\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "from wordcloud import WordCloud\n",
    "from fractions import Fraction\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import Model\n",
    "#!pip install -U -q keras_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"amazontelefondata.csv\" , engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0  \n",
       "1  Due to a software issue between Nokia and Spri...          17.0  \n",
       "2  This is a great, reliable phone. I also purcha...           5.0  \n",
       "3  I love the phone and all, because I really did...           1.0  \n",
       "4  The phone has been great for every purpose it ...           1.0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title']= data['title'].astype('str')\n",
    "data['body']=data['body'].astype('str')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>B0029F2O3A</td>\n",
       "      <td>n. country boy</td>\n",
       "      <td>4</td>\n",
       "      <td>October 6, 2010</td>\n",
       "      <td>False</td>\n",
       "      <td>never had a problem</td>\n",
       "      <td>never had a problem have had this phone for ov...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>B0029F2O3A</td>\n",
       "      <td>Kyle Griswold</td>\n",
       "      <td>1</td>\n",
       "      <td>October 29, 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>Terrible</td>\n",
       "      <td>Has some real ram issues. It is really unusabl...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>B0029F2O3A</td>\n",
       "      <td>Seth B. Tropinsky</td>\n",
       "      <td>5</td>\n",
       "      <td>July 27, 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>Wonderful innovative keypad</td>\n",
       "      <td>Great phone, which has the comfort of a larger...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B0029F2O3A</td>\n",
       "      <td>A-la Dawn</td>\n",
       "      <td>5</td>\n",
       "      <td>March 30, 2011</td>\n",
       "      <td>False</td>\n",
       "      <td>Best fone ever!!!</td>\n",
       "      <td>I have had this fone for two years, and have n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>B0029F2O3A</td>\n",
       "      <td>B. Hopper</td>\n",
       "      <td>5</td>\n",
       "      <td>September 20, 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>Great Phone!</td>\n",
       "      <td>I just got this phone a couple days ago and I ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin               name  rating                date  verified  \\\n",
       "0   B0000SX2UC              Janet       3    October 11, 2005     False   \n",
       "1   B0000SX2UC         Luke Wyatt       1     January 7, 2004     False   \n",
       "2   B0000SX2UC             Brooke       5   December 30, 2003     False   \n",
       "3   B0000SX2UC      amy m. teague       3      March 18, 2004     False   \n",
       "4   B0000SX2UC      tristazbimmer       4     August 28, 2005     False   \n",
       "..         ...                ...     ...                 ...       ...   \n",
       "95  B0029F2O3A     n. country boy       4     October 6, 2010     False   \n",
       "96  B0029F2O3A      Kyle Griswold       1    October 29, 2009     False   \n",
       "97  B0029F2O3A  Seth B. Tropinsky       5       July 27, 2009     False   \n",
       "98  B0029F2O3A          A-la Dawn       5      March 30, 2011     False   \n",
       "99  B0029F2O3A          B. Hopper       5  September 20, 2009     False   \n",
       "\n",
       "                                           title  \\\n",
       "0                    Def not best, but not worst   \n",
       "1                    Text Messaging Doesn't Work   \n",
       "2                                Love This Phone   \n",
       "3                        Love the Phone, BUT...!   \n",
       "4   Great phone service and options, lousy case!   \n",
       "..                                           ...   \n",
       "95                           never had a problem   \n",
       "96                                      Terrible   \n",
       "97                   Wonderful innovative keypad   \n",
       "98                             Best fone ever!!!   \n",
       "99                                  Great Phone!   \n",
       "\n",
       "                                                 body  helpfulVotes  \n",
       "0   I had the Samsung A600 for awhile which is abs...           1.0  \n",
       "1   Due to a software issue between Nokia and Spri...          17.0  \n",
       "2   This is a great, reliable phone. I also purcha...           5.0  \n",
       "3   I love the phone and all, because I really did...           1.0  \n",
       "4   The phone has been great for every purpose it ...           1.0  \n",
       "..                                                ...           ...  \n",
       "95  never had a problem have had this phone for ov...           NaN  \n",
       "96  Has some real ram issues. It is really unusabl...           2.0  \n",
       "97  Great phone, which has the comfort of a larger...           2.0  \n",
       "98  I have had this fone for two years, and have n...           1.0  \n",
       "99  I just got this phone a couple days ago and I ...           1.0  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\whosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\whosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\whosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "punc = set(string.punctuation)\n",
    "keywords = data[\"body\"].apply(lambda x: x.lower()).unique().tolist()\n",
    "keywords.append(\"phone\")\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = text.lower()\n",
    "   \n",
    "    wordList = text.split()\n",
    "   \n",
    "    wordList = [\"\".join(x for x in word if (x==\"'\")|(x not in punc)) for word in wordList]\n",
    "   \n",
    "    wordList = [word for word in wordList if word not in stop]\n",
    "    \n",
    "    wordList = [word for word in wordList if word not in keywords]\n",
    "    \n",
    "    wordList = [lemma.lemmatize(word) for word in wordList]\n",
    "    return \" \".join(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I had the Samsung A600 for awhile which is abs...\n",
       "1    Due to a software issue between Nokia and Spri...\n",
       "2    This is a great, reliable phone. I also purcha...\n",
       "3    I love the phone and all, because I really did...\n",
       "4    The phone has been great for every purpose it ...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin            67986\n",
       "name            67984\n",
       "rating          67986\n",
       "date            67986\n",
       "verified        67986\n",
       "title           67986\n",
       "body            67986\n",
       "helpfulVotes    27215\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_review'] =data['body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samsung a600 awhile absolute doo doo read revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>due software issue nokia sprint phone's text m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>reliable also purchased samsung a460 died menu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>really one expect bill received one also i've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>every purpose offer except day bought iti coul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   \n",
       "3  I love the phone and all, because I really did...           1.0   \n",
       "4  The phone has been great for every purpose it ...           1.0   \n",
       "\n",
       "                                    processed_review  \n",
       "0  samsung a600 awhile absolute doo doo read revi...  \n",
       "1  due software issue nokia sprint phone's text m...  \n",
       "2  reliable also purchased samsung a460 died menu...  \n",
       "3  really one expect bill received one also i've ...  \n",
       "4  every purpose offer except day bought iti coul...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_neg'] = [1 if x > 3 else 0 for x in data.rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samsung a600 awhile absolute doo doo read revi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>due software issue nokia sprint phone's text m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>reliable also purchased samsung a460 died menu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin        name  rating               date  verified  \\\n",
       "0  B0000SX2UC       Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC  Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC      Brooke       5  December 30, 2003     False   \n",
       "\n",
       "                         title  \\\n",
       "0  Def not best, but not worst   \n",
       "1  Text Messaging Doesn't Work   \n",
       "2              Love This Phone   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   \n",
       "\n",
       "                                    processed_review  pos_neg  \n",
       "0  samsung a600 awhile absolute doo doo read revi...        0  \n",
       "1  due software issue nokia sprint phone's text m...        0  \n",
       "2  reliable also purchased samsung a460 died menu...        1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    46576\n",
       "0    21410\n",
       "Name: pos_neg, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos_neg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">processed_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_neg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21410</td>\n",
       "      <td>19536</td>\n",
       "      <td></td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46576</td>\n",
       "      <td>34198</td>\n",
       "      <td></td>\n",
       "      <td>6562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        processed_review                 \n",
       "                   count unique top  freq\n",
       "pos_neg                                  \n",
       "0                  21410  19536       389\n",
       "1                  46576  34198      6562"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data[[\"pos_neg\",\"processed_review\"]]\n",
    "data2.groupby('pos_neg').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = data[\"processed_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.processed_review, data.pos_neg, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50989,)\n",
      "y_train shape: (50989,)\n",
      "\n",
      "x_test shape: (16997,)\n",
      "y_test shape: (16997,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape: {}\".format(x_train.shape), end='\\n')\n",
    "print(\"y_train shape: {}\".format(y_train.shape), end='\\n\\n')\n",
    "print(\"x_test shape: {}\".format(x_test.shape), end='\\n')\n",
    "print(\"y_test shape: {}\".format(y_test.shape), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10123    bought galaxy s6 considered allaround market i...\n",
       "46526    generali'm wih seller write chip housing that'...\n",
       "44836    absolutely flawless back black line way right ...\n",
       "34519                               wanted att got tmobile\n",
       "48881    much better expected delighted oneplus maintai...\n",
       "                               ...                        \n",
       "41993                                                     \n",
       "21243    one would think important thing sim card inclu...\n",
       "45891       got exactly ordered took little delivered wait\n",
       "42613    arrived faster expected set easy look function...\n",
       "43567    lot update definitely help usability camera fa...\n",
       "Name: processed_review, Length: 50989, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<50989x8924 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 948749 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=5).fit(x_train)\n",
    "X_train = vectorizer.transform(x_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Özellik sayısı: 8924\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Özellik sayısı: {}\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bazı özellik isimleri : \n",
      " ['001', 'award', 'correspond', 'excels', 'iam', 'mexico', 'potential', 'segãºn', 'tied']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bazı özellik isimleri : \\n\", vectorizer.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "scores = cross_val_score(MultinomialNB(), X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama çapraz doğrulama değerim: 0.844\n"
     ]
    }
   ],
   "source": [
    "print(\"Ortalama çapraz doğrulama değerim: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Tahmin edilen sonuca yazdırdım modeli\n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "predictions = mnb.predict(X_test)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sayısı : 8924 \n",
      "\n",
      "Bazılarının adı : \n",
      " ['001', 'award', 'correspond', 'excels', 'iam', 'mexico', 'potential', 'segãºn', 'tied']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whosa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidVectorızer kullanarak cok sık gorulen kelımelerın anlamsız olanların etkısını azalttım.\n",
    "tfidf = TfidfVectorizer(min_df=5) #belge sıklıgı\n",
    "X_train_tfidf = tfidf.fit_transform(x_train)\n",
    "print(\"Sayısı : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Bazılarının adı : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "En küçük özelliğe sahıp 10 özellik :\n",
      "['worst' 'waste' 'worse' 'disappointed' 'returning' 'refund' 'defective'\n",
      " 'poor' 'stopped' 'locked']\n",
      "\n",
      "En büyük özelliğe sahıp 10 özellik : \n",
      "['far' 'easy' 'glad' 'love' 'complaint' 'pleased' 'highly' 'exactly'\n",
      " 'value' 'minor']\n"
     ]
    }
   ],
   "source": [
    "#En kucuk ve en buyuk 10 ozellıge baktım\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nEn küçük özelliğe sahıp 10 özellik :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('En büyük özelliğe sahıp 10 özellik : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8555\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75      5374\n",
      "           1       0.86      0.94      0.90     11623\n",
      "\n",
      "    accuracy                           0.86     16997\n",
      "   macro avg       0.85      0.81      0.82     16997\n",
      "weighted avg       0.85      0.86      0.85     16997\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3638  1736]\n",
      " [  720 10903]]\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(tfidf.transform(x_test))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tekrardan matrise dönusturudm\n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(x_train)\n",
    "\n",
    "# SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "En küçük özelliğe sahıp 10 özellik :\n",
      "['worst' 'disappointed' 'waste' 'refund' 'returning' 'worse' 'stopped'\n",
      " 'defective' 'poor' 'locked']\n",
      "\n",
      "En büyük özelliğe sahıp 10 özellik : \n",
      "['far' 'easy' 'complaint' 'pleased' 'glad' 'little' 'exactly' 'highly'\n",
      " 'bit' 'quick']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('\\nEn küçük özelliğe sahıp 10 özellik :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('En büyük özelliğe sahıp 10 özellik : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8486\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73      5374\n",
      "           1       0.85      0.94      0.89     11623\n",
      "\n",
      "    accuracy                           0.85     16997\n",
      "   macro avg       0.85      0.79      0.81     16997\n",
      "weighted avg       0.85      0.85      0.84     16997\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3472  1902]\n",
      " [  671 10952]]\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(tfidf.transform(x_test))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\whosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Temişlenmiş hale getirdim\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #html sil\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # non-character sil\n",
    "    words = letters_only.lower().split() # lower yap.\n",
    "    \n",
    "    if remove_stopwords: # stopwords sil\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "#        \n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  #cümleleri bölme\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45775 eğitim kümesindeki çozumlenmıs kume\n",
      "\n",
      "Örnek ayrılmıs cümle : \n",
      " ['unloked', 'att']\n"
     ]
    }
   ],
   "source": [
    "# Cümlelere bölme yaptım\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Metni cümlelere ayrıştırın\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Eğitim setindeki her incelemeyi cümlelere ayırma\n",
    "sentences = []\n",
    "for review in x_train:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "    \n",
    "print('%d eğitim kümesindeki çozumlenmıs kume\\n'  %len(sentences))\n",
    "print('Örnek ayrılmıs cümle : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10123    bought galaxy s6 considered allaround market i...\n",
       "46526    generali'm wih seller write chip housing that'...\n",
       "44836    absolutely flawless back black line way right ...\n",
       "34519                               wanted att got tmobile\n",
       "48881    much better expected delighted oneplus maintai...\n",
       "                               ...                        \n",
       "41993                                                     \n",
       "21243    one would think important thing sim card inclu...\n",
       "45891       got exactly ordered took little delivered wait\n",
       "42613    arrived faster expected set easy look function...\n",
       "43567    lot update definitely help usability camera fa...\n",
       "Name: processed_review, Length: 50989, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model \n",
      "\n",
      "Listede ki kelime sayısı : 8629 \n",
      "\n",
      "10 tane örnek : \n",
      " ['s', 'i', 'one', 'use', 'get', 'would', 'time', 'phone', 'day', 'samsung']\n"
     ]
    }
   ],
   "source": [
    "# Ayrıştırılmış cümleleri Word2Vec modeline uydurma\n",
    "num_features = 100  #embedding boyutu                   \n",
    "min_word_count = 5                \n",
    "num_workers = 4       \n",
    "context = 5                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model \\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\n",
    "                window = context)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"eğitimliword2v\") #kaydet\n",
    "\n",
    "print(\"Listede ki kelime sayısı : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
    "print(\"10 tane örnek : \\n\", w2v.wv.index2word[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 20000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "nb_epoch = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(x_train)\n",
    "# acces sözlükten aldım egitilmis sözlük\n",
    "\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Kerasın modele uymaıs ıcın sıcak kod? \n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50989, 100)\n",
      "X_test shape: (16997, 100)\n",
      "y_train shape: (50989, 3)\n",
      "y_test shape: (16997, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train_seq.shape)\n",
    "print('X_test shape:', X_test_seq.shape) \n",
    "print('y_train shape:', y_train_seq.shape) \n",
    "print('y_test shape:', y_test_seq.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 384, 1474,  377, ...,  704,  133,   42],\n",
       "       [   0,    0,    0, ...,  180,   77,  153],\n",
       "       [   0,    0,    0, ...,  451,  979, 7677],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   69,  721,  479],\n",
       "       [   0,    0,    0, ...,   18, 2428,  260],\n",
       "       [   0,    0,    0, ...,   26,   23,   61]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq #birleştirip gösterdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,691,971\n",
      "Trainable params: 2,691,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1594/1594 [==============================] - 233s 143ms/step - loss: 0.3176 - accuracy: 0.7848\n",
      "Epoch 2/3\n",
      "1594/1594 [==============================] - 228s 143ms/step - loss: 0.1933 - accuracy: 0.8856\n",
      "Epoch 3/3\n",
      "1594/1594 [==============================] - 234s 147ms/step - loss: 0.1612 - accuracy: 0.9072\n",
      "532/532 [==============================] - 19s 35ms/step - loss: 0.2304 - accuracy: 0.8609\n",
      "Test kaybım : 0.2304\n",
      "Test doğruluğu: 0.8609\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128))\n",
    "model1.add(LSTM(128)) \n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "# LSTM derleniyor\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
    "\n",
    "# Model Evaluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test kaybım : {:.4f}'.format(score[0]))\n",
    "print('Test doğruluğu: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samsung a600 awhile absolute doo doo read revi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   name  rating              date  verified  \\\n",
       "0  B0000SX2UC  Janet       3  October 11, 2005     False   \n",
       "\n",
       "                         title  \\\n",
       "0  Def not best, but not worst   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "\n",
       "                                    processed_review  pos_neg  \n",
       "0  samsung a600 awhile absolute doo doo read revi...        0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, LSTM, GRU, Input,Bidirectional\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D,MaxPooling1D\n",
    "from keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer, hashing_trick\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove-python\n",
      "  Using cached glove_python-0.1.0.tar.gz (263 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from glove-python) (1.19.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from glove-python) (1.5.0)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py): started\n",
      "  Building wheel for glove-python (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for glove-python\n",
      "Failed to build glove-python\n",
      "Installing collected packages: glove-python\n",
      "    Running setup.py install for glove-python: started\n",
      "    Running setup.py install for glove-python: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\whosa\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-wheel-kp2u_gx5'\n",
      "       cwd: C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-install-4w2p65c1\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\n",
      "  Complete output (12 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\glove\n",
      "  copying glove\\corpus.py -> build\\lib.win-amd64-3.8\\glove\n",
      "  copying glove\\glove.py -> build\\lib.win-amd64-3.8\\glove\n",
      "  copying glove\\__init__.py -> build\\lib.win-amd64-3.8\\glove\n",
      "  running build_ext\n",
      "  building 'glove.glove_cython' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for glove-python\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\whosa\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-install-4w2p65c1\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\n",
      "  Complete output (6 lines):\n",
      "  usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "     or: setup.py --help [cmd1 cmd2 ...]\n",
      "     or: setup.py --help-commands\n",
      "     or: setup.py cmd --help\n",
      "  \n",
      "  error: option --all not recognized\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed cleaning build dir for glove-python\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\users\\whosa\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-record-_x5n8tnz\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\whosa\\anaconda3\\Include\\glove-python'\n",
      "         cwd: C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-install-4w2p65c1\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\n",
      "    Complete output (6 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    running build_ext\n",
      "    building 'glove.glove_cython' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\users\\whosa\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\whosa\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4w2p65c1\\\\glove-python_b7dc83d27dbc4d699cf89a19c51093f6\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\whosa\\AppData\\Local\\Temp\\pip-record-_x5n8tnz\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\whosa\\anaconda3\\Include\\glove-python' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3811369e9954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install glove-python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "!pip install glove-python\n",
    "from glove import Glove\n",
    "from glove import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          'Text of the first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# modele bölünmüş cümleleri aktarmamız gere\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "lines=tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'machine', 'learning', 'book'],\n",
    "            ['one', 'more', 'new', 'book'],\n",
    "         \n",
    "          ['this', 'is', 'about', 'machine', 'learning', 'post'],\n",
    "          ['orange', 'juice', 'is', 'the', 'liquid', 'extract', 'of', 'fruit'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'varieties'],\n",
    "          ['this', 'is', 'the', 'last', 'machine', 'learning', 'book'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'packages'],\n",
    "          ['orange', 'juice', 'is', 'liquid', 'extract', 'from', 'fruit', 'on', 'orange', 'tree']]\n",
    "lines=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-b70c0c3af523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#glove kitaplıgını ıce aktarma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# corpus nesnesi olusturdum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "import glove\n",
    "#glove kitaplıgını ıce aktarma\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# corpus nesnesi olusturdum\n",
    "corpus = Corpus() \n",
    "\n",
    "#GloVe'de kullanılan eş oluşum matrisini oluşturmak için külliyatın eğittim\n",
    "corpus.fit(lines, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install glove-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-ae8b3e0ce251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove2582020.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove' is not defined"
     ]
    }
   ],
   "source": [
    "glove = glove(no_components=30, learning_rate=0.05)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('amazondataglovemodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.25.3.tar.gz (25.3 MB)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (3.2.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (1.0.5)\n",
      "Requirement already satisfied: requests in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (2.24.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (0.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (20.4)\n",
      "Requirement already satisfied: ipython in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (7.16.1)\n",
      "Requirement already satisfied: networkx>=2.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (2.4)\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (2.1.0)\n",
      "Collecting seqeval==0.0.19\n",
      "  Downloading seqeval-0.0.19.tar.gz (30 kB)\n",
      "Requirement already satisfied: Keras>=2.2.4 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from seqeval==0.0.19->ktrain) (2.4.3)\n",
      "Collecting fastprogress>=0.1.21\n",
      "  Downloading fastprogress-1.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (2.10.0)\n",
      "Collecting keras_bert>=0.86.0\n",
      "  Downloading keras-bert-0.86.0.tar.gz (26 kB)\n",
      "Collecting keras-transformer>=0.38.0\n",
      "  Downloading keras-transformer-0.38.0.tar.gz (11 kB)\n",
      "Collecting keras-embed-sim>=0.8.0\n",
      "  Downloading keras-embed-sim-0.8.0.tar.gz (4.1 kB)\n",
      "Collecting keras-layer-normalization>=0.14.0\n",
      "  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\n",
      "Collecting keras-multi-head>=0.27.0\n",
      "  Downloading keras-multi-head-0.27.0.tar.gz (14 kB)\n",
      "Collecting keras-self-attention==0.46.0\n",
      "  Downloading keras-self-attention-0.46.0.tar.gz (10 kB)\n",
      "Collecting keras-pos-embd>=0.11.0\n",
      "  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\n",
      "Collecting keras-position-wise-feed-forward>=0.6.0\n",
      "  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->ktrain) (2020.1)\n",
      "Collecting transformers<4.0,>=3.1.0\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (4.47.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (3.0.12)\n",
      "Requirement already satisfied: protobuf in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (3.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (2020.6.8)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Collecting cchardet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Erişim engellendi: 'C:\\\\Users\\\\whosa\\\\anaconda3\\\\Lib\\\\site-packages\\\\~klearn\\\\datasets\\\\_svmlight_format_fast.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading cchardet-2.1.7-cp38-cp38-win_amd64.whl (115 kB)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.4.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (2.6.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (4.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (49.2.0.post20200714)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (3.0.5)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.17.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->ktrain) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ktrain) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (2020.6.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: click in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from sacremoses->transformers<4.0,>=3.1.0->ktrain) (7.1.2)\n",
      "Collecting syntok\n",
      "  Downloading syntok-1.3.1.tar.gz (23 kB)\n",
      "Collecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Building wheels for collected packages: ktrain, seqeval, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-self-attention, keras-pos-embd, keras-position-wise-feed-forward, jieba, langdetect, sacremoses, syntok\n",
      "  Building wheel for ktrain (setup.py): started\n",
      "  Building wheel for ktrain (setup.py): finished with status 'done'\n",
      "  Created wheel for ktrain: filename=ktrain-0.25.3-py3-none-any.whl size=25276311 sha256=7f3c909689ea09b0de7becabd3c6e9ccdd0ddb392cf363306be875357d65de6f\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\60\\76\\fc\\44c88639154def68ff86636b4579912be139bb89da3c8a1887\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.19-py3-none-any.whl size=9918 sha256=1c8c9a53c9401cfb02e0f38527d7afb97380c2a50c444d5fd74ed34c2bf6f16d\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\58\\e5\\9e\\2f94312239a6753267da6ceafe12d614603b78d308676df5e2\n",
      "  Building wheel for keras-bert (setup.py): started\n",
      "  Building wheel for keras-bert (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-bert: filename=keras_bert-0.86.0-py3-none-any.whl size=34143 sha256=42d74c545c2fbc98734b8d4192f1d4191177d103b0503bc44df9ecedb79b7ebb\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\d8\\69\\eb\\f4e8fda390b2599cde780fce1361c142367c00a8b98c82095e\n",
      "  Building wheel for keras-transformer (setup.py): started\n",
      "  Building wheel for keras-transformer (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-py3-none-any.whl size=12941 sha256=6684dcd09352b8a58f4927c694d49f9cdaa84fa9296346c4698c62d4fc70a97b\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\a9\\fb\\18\\ed70d4af00ce43f0ea7fbade107fe2f6e26bf27f24e08bfa7d\n",
      "  Building wheel for keras-embed-sim (setup.py): started\n",
      "  Building wheel for keras-embed-sim (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-py3-none-any.whl size=4557 sha256=6fab317b33bc0a61e352fa5f8d7bf3c76b5450cc95c1bf44353fe1bc538613c3\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\02\\f9\\ff\\71b8118e82790efd55bb33cbe2d21abeec3965cf8f36f5f84c\n",
      "  Building wheel for keras-layer-normalization (setup.py): started\n",
      "  Building wheel for keras-layer-normalization (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=88cb16b4106097adc91577717072baba9f8e247e15781cb5e15ca909bff4e5a0\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\02\\6a\\69\\04961d77d187981dec03832babe68ac9c9f559950bb8f62907\n",
      "  Building wheel for keras-multi-head (setup.py): started\n",
      "  Building wheel for keras-multi-head (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-py3-none-any.whl size=15615 sha256=9e96010cb2cdf1b8d76f14cf5712d934649f5d0e0e8209486e9130b67f3c6c24\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\89\\78\\6d\\6eb993e6ee85006e88e7d2bf954d4ad4893a573e0dc664eaf2\n",
      "  Building wheel for keras-self-attention (setup.py): started\n",
      "  Building wheel for keras-self-attention (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-py3-none-any.whl size=17277 sha256=e3c3529b599fd6340212b21b844aaedee38cb54dfd440257950fa2044ac6580b\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\0c\\62\\49\\4acbecf2af103fee5213fd22671d73614cd273a7baf4bd601c\n",
      "  Building wheel for keras-pos-embd (setup.py): started\n",
      "  Building wheel for keras-pos-embd (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7552 sha256=a0fc3b4e63a84f43d98e10d89a150f37b8b57385974c73e459c607d6f28db747\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\ad\\04\\9a\\b68e9d44d531ae48796ee329b1b93b7cb511162c71f2f6df03\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): started\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=1f2e4d222d532fb046ead481efd0ec3745bcf0b90928772f24e52adae25736a7\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\18\\f0\\90\\017a1e331356edfb4c81f0d34cb241d67ab590c9fdb685ada0\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=0b136311a73f25462965c8cfbcd5fd8fc124824cf458a7dadb7ce5bbe1590a1d\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\ca\\38\\d8\\dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993190 sha256=c5859728f124f6ceb530a52bd1f0a891555b94b24f6ceb3b4424c2628dce9631\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\1e\\80\\23\\0a24928ec3a3906ff5027f38d2fea824e7e97f2ba7c83d91e3\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=6ce28d9a855583c8d0287ab6543cad1498d8054d9e8d8c9592f5fb83474fa77a\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\7b\\78\\f4\\27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "  Building wheel for syntok (setup.py): started\n",
      "  Building wheel for syntok (setup.py): finished with status 'done'\n",
      "  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=da74687de7bcf4bf1ccd3f29c0a45a8ec4f95876ec77c661207cb58f259523e2\n",
      "  Stored in directory: c:\\users\\whosa\\appdata\\local\\pip\\cache\\wheels\\24\\fd\\23\\26e82d723a0455d2cb72fe6af9dd1f0b2a9a1312829470b304\n",
      "Successfully built ktrain seqeval keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-self-attention keras-pos-embd keras-position-wise-feed-forward jieba langdetect sacremoses syntok\n",
      "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sentencepiece, sacremoses, keras-transformer, whoosh, transformers, syntok, seqeval, scikit-learn, langdetect, keras-bert, jieba, fastprogress, cchardet, ktrain\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.1\n",
      "    Uninstalling scikit-learn-0.23.1:\n",
      "      Successfully uninstalled scikit-learn-0.23.1\n"
     ]
    }
   ],
   "source": [
    "! pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samsung a600 awhile absolute doo doo read revi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>due software issue nokia sprint phone's text m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>reliable also purchased samsung a460 died menu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>really one expect bill received one also i've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>every purpose offer except day bought iti coul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   \n",
       "3  I love the phone and all, because I really did...           1.0   \n",
       "4  The phone has been great for every purpose it ...           1.0   \n",
       "\n",
       "                                    processed_review  pos_neg  \n",
       "0  samsung a600 awhile absolute doo doo read revi...        0  \n",
       "1  due software issue nokia sprint phone's text m...        0  \n",
       "2  reliable also purchased samsung a460 died menu...        1  \n",
       "3  really one expect bill received one also i've ...        0  \n",
       "4  every purpose offer except day bought iti coul...        1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = ' '.join(data['processed_review'])\n",
    "text = text.split()\n",
    "freq_comm = pd.Series(text).value_counts()\n",
    "rare = freq_comm[freq_comm.values == 1]\n",
    "\n",
    "def get_clean_text(x):\n",
    "    if type(x) is str:\n",
    "        x = x.lower()\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) \n",
    "        #regex to remove to emails\n",
    "        x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
    "        #regex to remove URLs\n",
    "        x = re.sub('RT', \"\", x)\n",
    "        #substitute the 'RT' retweet tags with empty spaces\n",
    "        x = re.sub('[^A-Z a-z]+', '', x)\n",
    "        #combining all the text excluding rare words.\n",
    "        x = ' '.join([t for t in x.split() if t not in rare])\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "data['processed_review'] = data['processed_review'].apply(lambda x: get_clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>samsung a awhile absolute doo doo read review ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>due software issue nokia sprint phones text me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>reliable also purchased samsung a died menu ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>really one expect bill received one also have ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>every purpose offer except day bought iti coul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   \n",
       "3  I love the phone and all, because I really did...           1.0   \n",
       "4  The phone has been great for every purpose it ...           1.0   \n",
       "\n",
       "                                    processed_review  pos_neg  \n",
       "0  samsung a awhile absolute doo doo read review ...        0  \n",
       "1  due software issue nokia sprint phones text me...        0  \n",
       "2  reliable also purchased samsung a died menu ea...        1  \n",
       "3  really one expect bill received one also have ...        0  \n",
       "4  every purpose offer except day bought iti coul...        1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8b06abc4194b92892b39e037e03709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 23\n",
      "\t95percentile : 85\n",
      "\t99percentile : 228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc5fbb73fb24570a47e9c4a27949b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f34a05f158c4679aa06d718ec0357dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 23\n",
      "\t95percentile : 85\n",
      "\t99percentile : 228\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-5338afd92fb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m (X_train,y_train),(X_test,y_test),preproc =text.texts_from_df(train_df= data,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                                              \u001b[0mtext_column\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'processed_review'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                              \u001b[0mlabel_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pos_neg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                              \u001b[0mval_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                              \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "(X_train,y_train),(X_test,y_test),preproc =text.texts_from_df(train_df= data,\n",
    "                                                             text_column ='processed_review',\n",
    "                                                             label_columns = 'pos_neg',\n",
    "                                                             val_df = data ,\n",
    "                                                             maxlen = 512 ,\n",
    "                                                             preprocess_mode = 'distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Using cached ktrain-0.25.3-py3-none-any.whl\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (0.1.91)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (3.2.2)\n",
      "Requirement already satisfied: syntok in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (1.3.1)\n",
      "Requirement already satisfied: networkx>=2.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (2.4)\n",
      "Requirement already satisfied: ipython in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (7.16.1)\n",
      "Requirement already satisfied: transformers<4.0,>=3.1.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (3.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (2.24.0)\n",
      "Requirement already satisfied: whoosh in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (2.7.4)\n",
      "Requirement already satisfied: scikit-learn==0.23.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (0.23.2)\n",
      "Requirement already satisfied: seqeval==0.0.19 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (0.0.19)\n",
      "Requirement already satisfied: joblib in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (0.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (20.4)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ktrain) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from scikit-learn==0.23.2->ktrain) (2.1.0)\n",
      "Requirement already satisfied: Keras>=2.2.4 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from seqeval==0.0.19->ktrain) (2.4.3)\n",
      "Collecting fastprogress>=0.1.21\n",
      "  Using cached fastprogress-1.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (5.3.1)\n",
      "Collecting keras-bert>=0.86.0\n",
      "  Using cached keras_bert-0.86.0-py3-none-any.whl\n",
      "Requirement already satisfied: keras-transformer>=0.38.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-bert>=0.86.0->ktrain) (0.38.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.6.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.14.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.14.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.27.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.27.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.8.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.8.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.11.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: keras-self-attention==0.46.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.46.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->ktrain) (2020.1)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (0.9.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (4.47.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers<4.0,>=3.1.0->ktrain) (3.14.0)\n",
      "Collecting cchardet\n",
      "  Using cached cchardet-2.1.7-cp38-cp38-win_amd64.whl (115 kB)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (3.0.5)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (49.2.0.post20200714)\n",
      "Requirement already satisfied: colorama in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.4.3)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.17.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from ipython->ktrain) (4.3.3)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->ktrain) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ktrain) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->ktrain) (2020.6.20)\n",
      "Requirement already satisfied: click in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from sacremoses->transformers<4.0,>=3.1.0->ktrain) (7.1.2)\n",
      "Installing collected packages: langdetect, keras-bert, jieba, fastprogress, cchardet, ktrain\n",
      "Successfully installed cchardet-2.1.7 fastprogress-1.0.0 jieba-0.42.1 keras-bert-0.86.0 ktrain-0.25.3 langdetect-1.0.8\n"
     ]
    }
   ],
   "source": [
    "! pip install ktrain\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-0f7e03a00086>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model =text.text_classifier(name = 'distilbert',\n\u001b[0m\u001b[0;32m      3\u001b[0m                             \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             preproc =preproc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "model =text.text_classifier(name = 'distilbert',\n",
    "                            train_data = (X_train,y_train),\n",
    "                            preproc =preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT DENEMESİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\whosa\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from pandas) (1.19.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Collecting torch\n",
      "  Downloading torch-1.7.1-cp38-cp38-win_amd64.whl (184.0 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from torch) (1.19.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.7.1\n",
      "Requirement already satisfied: transformers in c:\\users\\whosa\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (1.19.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\whosa\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install torch\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import  BertModel, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_shuffle(file):\n",
    "    df = pd.read_csv(file, delimiter=',')\n",
    "    # Rastgele karıştırma\n",
    "    df.sample(frac=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_val_split(df, splitRatio=0.8):\n",
    "    train=df.sample(frac=splitRatio,random_state=200)\n",
    "    val=df.drop(train.index)\n",
    "    print(\"Eğitim Örneği Sayım: \", len(train))\n",
    "    print(\"Doğrulama Örneği Sayım: \", len(val))\n",
    "    return(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(reviews):\n",
    "    return len(max(reviews, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, labels):\n",
    "    # satırdaki maksimum değerin dizinini aldım.\n",
    "    predictedClass = logits.max(dim = 1)[1]\n",
    "\n",
    "    # tüm verinin ortalamasını alarak doğruluk elde ettim\n",
    "    acc = (predictedClass == labels).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFunc(net, loss_func, opti, train_loader, test_loader, config):\n",
    "    best_acc = 0\n",
    "    for ep in range(config[\"epochs\"]):\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            \n",
    "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
    "\n",
    "            logits = net(seq, attn_masks)\n",
    "            loss = loss_func(m(logits), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            print(\"Iteration: \", it+1)\n",
    "\n",
    "            if (it + 1) % config[\"printEvery\"] == 0:\n",
    "                acc = get_accuracy(m(logits), labels)\n",
    "                if not os.path.exists(config[\"outputFolder\"]):\n",
    "                    os.makedirs(config[\"outputFolder\"])\n",
    "\n",
    "                # Tek bir dönem saatlerce sürebileceğinden, eğitim doğruluğunun değerlendirilmesi sırasında bile modeli düzenli olarak kaydediyoruz.\n",
    "                torch.save(net.state_dict(), os.path.join(projectFolder, config[\"outputFolder\"], config[\"outputFileName\"]))\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
    "                print(\"Saving at\", os.path.join(projectFolder, config[\"outputFolder\"], config[\"outputFileName\"]))\n",
    "\n",
    "        # bir dönemin sonunda doğrulama gerçekleştirin.\n",
    "        val_acc, val_loss = evaluate(net, loss_func, val_loader, config)\n",
    "        print(\" Validation Accuracy : {}, Validation Loss : {}\".format(val_acc, val_loss))\n",
    "        if val_acc > best_acc:\n",
    "            print(\"En iyi doğrulama doğruluğu {} ile {} arasında iyileştirildi, model kaydediliyor ...\".format(best_acc, val_acc))\n",
    "            best_acc = val_acc\n",
    "            torch.save(net.state_dict(), os.path.join(projectFolder, config[\"outputFolder\"], config[\"outputFileName\"] + \"_valTested_\" + str(best_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loss_func, dataloader, config):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            \n",
    "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
    "\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += loss_func(m(logits), labels)\n",
    "            mean_acc += get_accuracy(m(logits), labels)\n",
    "            print(\"iterasyon değeri\", count+1)\n",
    "            count += 1\n",
    "\n",
    "            '''\n",
    "            Doğrulama setinin tamamı yaklaşık 0,1 milyon girişti,\n",
    "             validationFraction parametresi, karıştırılan öğenin ne kadarını kontrol eder\n",
    "             sonuçları doğrulamak istediğiniz doğrulama kümesi.\n",
    "            '''\n",
    "            if count > config[\"doğrulamafonksyonu\"] * len(val_set):\n",
    "                break\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"splitRatio\" : 0.8,\n",
    "    \"maxLength\" : 100,\n",
    "    \"printEvery\" : 100,\n",
    "    \"outputFolder\" : \"Models\",\n",
    "    \"outputFileName\" : \"AmazonReviewClassifier.dat\",\n",
    "    \"threads\" : 4,\n",
    "    \"batchSize\" : 64,\n",
    "    \"validationFraction\" : 0.0005,\n",
    "    \"epochs\" : 5,\n",
    "    \"forceCPU\" : False\n",
    "    }\n",
    "if config[\"forceCPU\"]:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "config[\"device\"] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, device, freeze_bert = True):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.device = device\n",
    "\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.cls_layer = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Girişler:\n",
    "             -seq: sekansların simge kimliklerini içeren şekil [B, T] tensörü\n",
    "             -attn_masks: PAD belirteçlerinin bulaşmasını önlemek için kullanılacak dikkat maskelerini içeren [B, T] şekil tensörü\n",
    "        '''\n",
    "\n",
    "        #Bağlamsallaştırılmış temsiller elde etmek için girdiyi BERT modeline besleme\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        \n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "       \n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(Dataset):\n",
    "    def __init__(self, df, maxlen):\n",
    "        self.df = df\n",
    "        # Yeniden dizini 1'den len (df) 'ye sıfırla, karıştırılmış df çerçeveleri seyrek.\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.loc[index, 'Text']\n",
    "\n",
    "        # Classes start from 0.\n",
    "        label = int(self.df.loc[index, 'Score']) - 1\n",
    "\n",
    "        # Use BERT tokenizer since it needs to be able to match the tokens to the pre trained words.\n",
    "        tokens = self.tokenizer.tokenize(review)\n",
    "\n",
    "        # BERT inputs typically start with a '[CLS]' tag and end with a '[SEP]' tag. For\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "        if len(tokens) < self.maxlen:\n",
    "            # Add the ['PAD'] token\n",
    "            tokens = tokens + ['[PAD]' for item in range(self.maxlen-len(tokens))]\n",
    "        else:\n",
    "            # Truncate the tokens at maxLen - 1 and add a '[SEP]' tag.\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
    "\n",
    "        # BERT tokenizer converts the string tokens to their respective IDs.\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Converting to pytorch tensors.\n",
    "        tokens_ids_tensor = torch.tensor(token_ids)\n",
    "\n",
    "        # Masks place a 1 if token != PAD else a 0.\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "        \n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Configuration is:  {'splitRatio': 0.8, 'maxLength': 100, 'printEvery': 100, 'outputFolder': 'Models', 'outputFileName': 'AmazonReviewClassifier.dat', 'threads': 4, 'batchSize': 64, 'validationFraction': 0.0005, 'epochs': 5, 'forceCPU': False, 'device': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print(\"Configuration is: \", config)\n",
    "# Read and shuffle input data.\n",
    "df = read_and_shuffle(os.path.join(\"amazontelefondata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Target Output Classes: 5\n"
     ]
    }
   ],
   "source": [
    "num_classes = df['rating'].nunique()\n",
    "print(\"Number of Target Output Classes:\", num_classes)\n",
    "totalDatasetSize = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the column Score. This helps you get distribution of the Review Scores.\n",
    "symbols = df.groupby('rating')\n",
    "\n",
    "scores_dist = []\n",
    "for i in range(num_classes):\n",
    "    scores_dist.append(len(symbols.groups[i+1])/totalDatasetSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples:  54389\n",
      "Number of Validation Samples:  13597\n"
     ]
    }
   ],
   "source": [
    "train, val = get_train_and_val_split(df, config[\"splitRatio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(os.path.join(\"amazontelefondata.csv\"))\n",
    "train.to_csv(os.path.join(\"amazontelefondata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set the length to the true max length from the dataset, I have reduced it for the sake of memory and quicker training.\n",
    "#T = get_max_length(reviews)\n",
    "T = config[\"maxLength\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = AmazonReviewsDataset(train, T)\n",
    "val_set = AmazonReviewsDataset(val, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = config[\"batchSize\"], num_workers = config[\"threads\"])\n",
    "val_loader = DataLoader(val_set, batch_size = config[\"batchSize\"], num_workers = config[\"threads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4965e2f78c6c4c98b014c3aa9ed2328d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad5042d3465487897f38f7dba7c0e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We are unfreezing the BERT layers so as to be able to fine tune and save a new BERT model that is specific to the Sizeable food reviews dataset.\n",
    "\n",
    "net = SentimentClassifier(num_classes, config[\"device\"], freeze_bert=False)\n",
    "net.to(config[\"device\"])\n",
    "weights = torch.tensor(scores_dist).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Loss function and Optimizer.\n",
    "loss_func = nn.NLLLoss(weight=weights)\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
    "m = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-21f1b80f4786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrainFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "trainFunc(net, loss_func, opti, train_loader, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pytorch'\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch=1.3.1 torchvision=0.4.2 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_setDevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1f42f14a8a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_setDevice'"
     ]
    }
   ],
   "source": [
    "torch._C._cuda_setDevice(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPclassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4a8a620ff566>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLPclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Count dogruluk oranı:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLPclassifier' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import seaborn as sbn\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import floor,ceil\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "\n",
    "from gensim import summarization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>tristazbimmer</td>\n",
       "      <td>4</td>\n",
       "      <td>August 28, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "      <td>The phone has been great for every purpose it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
       "\n",
       "                                          title  \\\n",
       "0                   Def not best, but not worst   \n",
       "1                   Text Messaging Doesn't Work   \n",
       "2                               Love This Phone   \n",
       "3                       Love the Phone, BUT...!   \n",
       "4  Great phone service and options, lousy case!   \n",
       "\n",
       "                                                body  helpfulVotes  \n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0  \n",
       "1  Due to a software issue between Nokia and Spri...          17.0  \n",
       "2  This is a great, reliable phone. I also purcha...           5.0  \n",
       "3  I love the phone and all, because I really did...           1.0  \n",
       "4  The phone has been great for every purpose it ...           1.0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    " def data_clean( rev, remove_stopwords=True): \n",
    "    \n",
    "\n",
    "    new_text = re.sub(\"[^a-zA-Z]\",\" \", rev)\n",
    "   \n",
    "    words = new_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        sts = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in sts]\n",
    "    ary=[]\n",
    "    eng_stemmer = english_stemmer \n",
    "    for word in words:\n",
    "        ary.append(eng_stemmer.stem(word))\n",
    "\n",
    "    \n",
    "    return(ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-fa0819878966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclean_reviewData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrev\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mclean_reviewData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-3e6e54498e31>\u001b[0m in \u001b[0;36mdata_clean\u001b[1;34m(rev, remove_stopwords)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m    \u001b[0mnew_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m    \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "clean_reviewData = []\n",
    "for rev in data['body']:\n",
    "    clean_reviewData.append( \" \".join(data_clean(rev)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
